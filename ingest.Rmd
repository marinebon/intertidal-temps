---
title: "ingest"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, message = F, warning = F)
```

# load functions
```{r}
source(here::here("functions.R"))
```

# MARINe sites

### read in site data
```{r}
# get [Pole to Pole](https://marinebon.org/p2p) sites & convert to sf 
d_psites <- read_csv(here::here("data/p2p_sites.csv"))

d_psites_sf <- d_psites %>% 
  st_as_sf(
    coords = c("lon", "lat"), remove = F,
    crs    = 4326) # geographic coordinate ref system

# get MARINe sites, prep for combining temp data txts, & convert to sf
# from robo metadata
d_msites <- read_csv("Robomussel metadata.csv") %>% 
  rename(
    microsite_id   = `microsite id`,
    logger_type    = `logger type`,
    tidal_height_m = `tidal height (m)`,
    wave_exposure  = `wave exposure`,
    start_date     = `start date`,
    end_date       = `end date`)

d_mfiles <- tibble(
  path            = list.files(dir_gdata, ".*\\.txt", full.names = T),
  file            = basename(path),
  microsite_year  = file %>% path_ext_remove()) %>% 
  separate(microsite_year, c("msite", "year"), "_", convert = T)

# join file names with their associated metadata
d_msites <- d_mfiles %>% 
  left_join(
    d_msites, by = c("msite" = "microsite_id")) %>% 
  drop_na() # sum(is.na(d_msites$site)): n = 2

# convert to sf for joining with p2p
d_msites_sf <- d_msites %>% 
  st_as_sf(
    coords = c("longitude", "latitude"), remove = F,
    crs    = 4326) # geographic coordinate ref system (WGS1984)
```

### join MARINe and p2p sites by nearest feature
```{r}
# join 4 nearby sites by nearest feature
# (sites that are both MARINe and p2p that we have data for)
sites_joined <- st_join(
  d_msites_sf, 
  d_psites_sf %>% 
    select(-country) %>% 
    rename(pgeometry = geometry), 
  join = st_nearest_feature)

# add MARINe & p2p geom columns,
# then use to calc distance between sites in km
sites_joined <- sites_joined %>% 
  st_drop_geometry() %>% 
  mutate(
    geom_marine = map2(longitude, latitude, xy2pt),
    geom_p2p    = map2(lon      , lat     , xy2pt),
    dist_km     = map2_dbl(geom_marine, geom_p2p, st_distance) / 1000) %>% 
  st_as_sf( 
    coords = c("lon", "lat"), remove = F,
    crs    = 4326)
```

### smooth MARINe sites 
```{r warning=FALSE}
d_filtered <- split(sites_joined, list(sites_joined$id, sites_joined$zone), drop = T, sep = "_")

# loop through all site & zone combinations and smooth 
# store dailyq for each in a list
dailyq <- list()

for (i in 1:length(d_filtered)){ # i = 1
  
  # combine data for all zones of each site
  d_site_zone <- dataCombiner(data_site_zone = d_filtered[[i]])
  
  # get site and zone names
  site <- unique(d_filtered[[i]][["id"]])
  if ("zone" %in% colnames(d_filtered[[i]])) {
    zone <- unique(d_filtered[[i]][["zone"]])
    site_zone <-  glue("{site}_{zone}")
  } else {
    zone <- ""
    site_zone <- site
  }
  message(glue("site_zone: {site_zone}"))
  
  # smooth data
  d_dailyq <- dailyQuantilesData(d_site_zone)
  
  stringname <- glue("{site_zone}_dailyq")
  
  # populate dailyq list
  dailyq[[stringname]] <- d_dailyq
  
}

# create vector of ordered zones for when we convert zones to factor
zones <- c("Low", "Lower-Mid", "Mid", "Upper-Mid", "High")

# bind temps for all sites & zones
d <- dailyq %>% 
  bind_rows %>% 
  mutate(
    site   = as.factor(site),
    zone   = factor(zone, levels = zones, ordered = T),
    metric = as.factor(metric),
    Temp_C = as.numeric(Temp_C))
 

# write smoothed avg temp data for each site to a unique csv
for (i in 1:length(levels(d$site))) { # for each site i
  
  site <- levels(d$site)[i]
  
  d_site <- d %>% 
    filter(site == !!site) %>%
    ungroup()
  
  # Filter out avgs
  d_site_avg <- d_site %>% 
    filter(metric == "temp_c_avg") %>% 
    select(-site, -metric) %>% 
    mutate(
      zone   = factor(zone, zones, ordered = T),
      Temp_C = as.numeric(Temp_C)) %>% 
    arrange(zone, day) %>% 
    pivot_wider(day, names_from = zone, values_from = Temp_C) %>% 
    arrange(day)
  
  write_csv(
    d_site_avg,
    here(glue("data_smoothed/{site}.csv")))
  
}
```

# p2p sites

### read p2p gdata and metadata
```{r warning=FALSE}
# prep directories
dir_p2p_robo <-
  case_when(
    user == "bbest"       ~ "/Volumes/GoogleDrive/My Drive/projects/mbon-p2p/Robolimpets",
    user == "cdobbelaere" ~ here::here("../Robolimpets"))

paths <- list.files(
  dir_p2p_robo, ".*\\.csv$", 
  recursive = T, full.names = T,
  include.dirs = T) # 14 CSVs


# read all csv files in 'robolimpets' folder and combine data in list column
d_p2p <- tibble()
d_p2p <- tibble(
  path     = paths,
  file     = basename(path),
  data     = map(path, read_tempcsv)) 

metadata <- tibble() 
metadata <- map(d_p2p$path, read_metadata) %>% 
  bind_rows

# fill NAs
for (i in 1:nrow(metadata)) {
  for (j in 1:ncol(metadata)) {
    # if there's an NA,
    if (is.na(metadata[i, j])) {
      # find if serial number matches any other serial number
      if (TRUE %in% grepl(as.character(metadata$`serial number`[i]), metadata$`serial number`)) {
        row <- (grep(as.character(metadata$`serial number`[i]), metadata$`serial number`))[1]
        # replace that row's NAs with the value of the associated row(s)
        metadata[i, j] <- metadata[row, j]
      }
    }
  }
}


metadata <- metadata %>% 
  mutate_at(vars(-`serial number`, -path, -zone), as.numeric) 

d_p2p <- metadata %>% 
  left_join(d_p2p, by = c("path" = "path"), keep = F) %>% 
  select(path, file, everything())

psites_gdata_sf <- d_p2p %>% 
  st_as_sf(
    coords = c("lon", "lat"), remove = F,
    crs    = 4326) # geographic coordinate ref system

# df with geometry, attributes, data, etc.
psites <- st_join(
  psites_gdata_sf, 
  d_psites_sf %>% 
    rename(pgeometry = geometry), 
  join = st_nearest_feature) %>% 
  rename(
    lat_logger = lat.x,
    lon_logger = lon.x, 
    lat_site   = lat.y,
    lon_site   = lon.y)

for (i in 1:nrow(psites)) {
  psites[["data"]][[i]]$site <- psites$id[i]
  if (is.na(psites[["data"]][[i]]$zone)) {
    psites[["data"]][[i]]$zone <- psites$`serial number`[i]
  }
}
``` 

### smooth p2p gdata 
```{r warning=FALSE}
# split by site & zone/serial number combo for combining
d_filtered <- split(psites[["data"]], list(psites$id, psites$zone), drop = T, sep = "_")

dailyq <- list()

for (i in 1:length(d_filtered)) {
  
  num_loggers    <- length(d_filtered[[i]])
  site           <- unique(d_filtered[[i]][[num_loggers]][["site"]])
  zone           <- unique(d_filtered[[i]][[num_loggers]][["zone"]])
  site_zone_name <- glue("{site}_{zone}_dailyq")
  
  message(glue("site_zone: {site}_{zone}"))
  
  d_filtered[[i]] <- d_filtered[[i]] %>% bind_rows() %>% group_by(zone)
  d_dailyq        <- dailyQuantilesData(d_filtered[[i]])
  
  dailyq[[site_zone_name]] <- d_dailyq
}

# bind temps for all sites & zones
d <- dailyq %>% 
  bind_rows %>% 
  mutate(
    site   = as.factor(site),
    zone   = as.factor(zone),
    metric = as.factor(metric))

# write smoothed temp data to csv and zones to txt if relevant
for (i in 1:length(levels(d$site))) { # for each site 

  site <- levels(d$site)[i]

  d_site <- d %>% 
    filter(site == !!site) %>% 
    ungroup() 
  
  zones <- unique(d_site$zone)

  d_site <- d_site %>% 
    select(-site, -zone, -path) %>% 
    mutate(Temp_C = as.numeric(Temp_C)) %>%
    arrange(day) %>% 
    pivot_wider(day, names_from = metric, values_from = Temp_C) 
  
  message(glue("writing csv for: {site}"))
  
  write_csv(d_site,  here(glue("data_smoothed/{site}.csv")))
  write_lines(zones, here(glue("data_smoothed/{site}_zones.txt")))
}
```

# organize report output 

### MARINe
```{r}
sites_joined_summarized <- sites_joined %>% 
  group_by(site) %>% 
  mutate(num_loggers = length(unique(msite))) %>% 
  distinct(site, .keep_all = T) %>% 
  select(site, location, id, name, region, country, org, num_loggers, lat, lon) 

msites_smoothed <- 
  tibble(
    path        = list.files(here(glue("{getwd()}/data_smoothed")),".*\\.csv$", recursive = T, full.names = T),
    file        = basename(path),
    site_id     = file %>% path_ext_remove()) %>% 
  right_join(
    sites_joined_summarized,
    by = c("site_id" = "id"), copy = F, keep = F) %>% 
  select(site_id, name, everything()) %>% 
  select(-geometry, -path) %>% 
  rename(id = site_id)

write_csv(msites_smoothed, glue("{getwd()}/data_smoothed/msites_smoothed.csv"))

# read metadata back in
m_info <- read_csv(here::here("data_smoothed/msites_smoothed.csv"))
    
# write yaml metadata files
for (id in m_info$id) {
  message(glue("id: {id}"))
  m_info %>% 
    filter(id == !!id) %>% 
    write_yaml(
      here(glue("data_smoothed/{id}_meta.yml")),
      fileEncoding = "UTF-8")
}
```

### p2p gdata
```{r}
psites_summarized <- psites %>% 
  mutate(num_sites = length(unique(id))) %>% 
  group_by(id) %>% 
  mutate(num_loggers = length(unique(`serial number`))) %>% 
  distinct(id, .keep_all = T) %>% 
  select(id, name, country, org, number_of_samples, num_loggers, lat_site, lon_site)
  
psites_smoothed <- 
  tibble(
    path = list.files(glue("{getwd()}/data_smoothed"), ".*\\.csv$", recursive = T, full.names = T),
    file = basename(path),
    id = file %>% path_ext_remove()) %>% 
  right_join(psites_summarized, by = c("id" = "id"), copy = F, keep = F) %>% 
  select(id, name, everything()) %>% 
  select(-geometry, -path)
 
write_csv(psites_smoothed, here(glue("data_smoothed/psites_smoothed.csv")))

# read metadata back in
p2p_info <- read_csv(here::here("data_smoothed/psites_smoothed.csv"))

# write yaml metadata files
for (id in p2p_info$id) {  # id = p2p_info$id[1]
  message(glue("id: {id}"))
  p2p_info %>% 
    filter(id == !!id) %>% 
    write_yaml(
      here(glue("data_smoothed/{id}_meta.yml")),
      fileEncoding = "UTF-8")
}
```
