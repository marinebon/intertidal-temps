---
title: "ingest"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, message = F, warning = F)
```

## TODO

1. PROBLEM. Data is too big for simple interactive time-series plot (ie dygraphs) if on same Rmd output html page. The folder of *.csv's [robomusseldata20201030 - Google Drive](https://drive.google.com/drive/u/3/folders/1kzjZ72vFxRGsafTgCCzq6cXIhQ1GYQa3) contains 317 files totalling 236 MB. SOLUTION(s):
    1. Read into SQLite database and show as Shiny app.

  

# Load packages & user info

```{r}
# libraries
if (!require(librarian)){
  install.packages("librarian")
  library(librarian)
}
librarian::shelf(
  # time-series
  caTools, tools, dygraphs, xts,
  #spatial
  sf, leaflet, leaflet.extras,
  # tidyverse
  fs, glue, here, lubridate, stringr, tidyverse, purrr)

# paths & variables
user <- Sys.info()[["user"]]
```


## Ingest single txt

```{r}
# set dir_gdata as filepath for robomussels data on Google Drive
dir_gdata <- case_when(
  #user == "bbest"       ~ "/Users/bbest/Downloads/robomusseldata20201030",
  user == "bbest"       ~ "/Volumes/GoogleDrive/My Drive/projects/mbon-p2p/data/rocky/MARINe/robomusseldata20201030",
  user == "cdobbelaere" ~ "/Users/cdobbelaere/Documents/robomussels/robomusseldata20201030")
dir_avg <- file.path(dirname(dir_gdata), "robomusseldata20201030_avg")
stopifnot(any(dir.exists(c(dir_gdata, dir_avg))))

# TODO: iterate over files in dir_gdata

# path individual tab-seperated value (*.tsv) file
tsv              <- file.path(dir_gdata, "BMRMUSCABD3_2012.txt")
csv_hourlymean   <- glue("{dir_avg}/{basename(path_ext_remove(tsv))}_hourlymean.csv")
csv_movingwindow <- glue("{dir_avg}/{basename(path_ext_remove(tsv))}_movingwindow.csv")
csv_dailyavg     <- glue("{dir_avg}/{basename(path_ext_remove(tsv))}_dailyavg.csv")
csv_dailyq       <- glue("{dir_avg}/{basename(path_ext_remove(tsv))}_dailyq.csv")

stopifnot(file.exists(tsv))


```

### Smoothing for each indiv logger txt file

#### 1. Hourly mean (from every 10 min)

```{r}
# create df containing hourly mean temp data
d_hourlymean <- d %>% 
  mutate(time = floor_date(time, unit = "hour")) %>% # round each time down to the nearest hourly boundary 
  # (could alternatively round up with ceiling_date() or round to nearest values with round_date())
  group_by(time) %>% # group by hour
  summarize(Temp_C_hourly_mean = mean(Temp_C)) # calculate mean for each hour

# show file size difference from original
# write_csv(d_hourlymean, csv_hourlymean)
# file_size(c(tsv, csv_hourlymean))

# convert to xts for dygraph
x_hourlymean <- d_hourlymean
x_hourlymean <- xts(select(x_hourlymean, -time), order.by=x_hourlymean$time) 

# output dygraph interactive plot
dygraph(x_hourlymean, main="Hourly_Mean_Temp_C") %>%
  dyOptions(
    colors = "red",
    fillGraph = TRUE, fillAlpha = 0.4) %>% 
  dyRangeSelector()
```

#### 2. Min/max over 6 hr moving window
(note: can do rolling averages directly in dygraphs too with dygraph() %>% dyRoller(rollPeriod = 6)

```{r}
# 6 hour moving average, using hourly averages from earlier
d_movingwindow <- d_hourlymean %>% 
  mutate(
    # min for 6 hr moving window, based on hourly mean
    Temp_C_min_06_hours  = runmin(Temp_C_hourly_mean,  k=6, alg="C", endrule="constant", align="center"),
    # mean for 6 hr moving window, based on hourly mean
    Temp_C_mean_06_hours = runmean(Temp_C_hourly_mean, k=6, alg="C", endrule="constant", align="center"),
    # max for 6 hr moving window, based on hourly mean
    Temp_C_max_06_hours  = runmax(Temp_C_hourly_mean,  k=6, alg="C", endrule="constant", align="center")
    ) %>% 
  select(-Temp_C_hourly_mean)
  
# show file size difference from original
write_csv(d_movingwindow, csv_movingwindow)
file_size(c(tsv, csv_movingwindow))

# convert to xts  
x_movingwindow <- xts(select(d_movingwindow, -time), order.by=d_movingwindow$time) 

# output dygraph plot
dygraph(x_movingwindow, main="Temp_C") %>%
  dySeries(
    c("Temp_C_min_06_hours",
      "Temp_C_mean_06_hours",
      "Temp_C_max_06_hours"),
    label = "Temp (ºC) over 6 hour moving window",
    color = "orangered") %>% 
  dyOptions(
    #colors = c("orange", "orangered", "red"),
    fillGraph = FALSE, fillAlpha = 0.4) %>% 
  dyRangeSelector() 

# is there a way to display min/max data labels when using min/max as upper/lower bars?
```

#### 3. Min/max over day

First find points of inflection to break up day. Possibly averaging around 3 points of max in case anomalous 

```{r}
d_dailyavg <- d %>% 
  mutate(
    day = floor_date(time, unit = "day")) %>%
  group_by(day) %>%
  summarize(
    temp_c_avg = mean(Temp_C),
    temp_c_min = min(Temp_C),
    temp_c_max = max(Temp_C))

# show file size difference from original
write_csv(d_dailyavg, csv_dailyavg)
file_size(c(tsv, csv_dailyavg))

# convert to xts for dygraph
x_dailyavg <- d_dailyavg
x_dailyavg <- xts(select(x_dailyavg, -day), order.by=x_dailyavg$day) 

dygraph(x_dailyavg, main="Daily Temperature (ºC)") %>%
  dySeries(
    c("temp_c_min",
      "temp_c_avg",
      "temp_c_max"),
    label = "Daily Temperature (ºC)",
    color = "orangered") %>% 
  dyOptions(
    fillGraph = FALSE, fillAlpha = 0.4) %>%
  dyRangeSelector() 
```

#### 4. Quantiles (& min/max) over day

```{r}
d_dailyq <- d %>% 
  mutate(
    day = floor_date(time, unit = "day")) %>%
  group_by(day) %>%
  summarize(
    temp_c_q10 = quantile(Temp_C, 0.1),
    temp_c_q90 = quantile(Temp_C, 0.9),
    temp_c_avg = mean(Temp_C),
    temp_c_min = min(Temp_C),
    temp_c_max = max(Temp_C))

# show file size difference from original
write_csv(d_dailyq, csv_dailyq)
file_size(c(tsv, csv_dailyq))

# convert to xts for dygraph
x_dailyq <- d_dailyq
x_dailyq <- xts(select(x_dailyq, -day), order.by=x_dailyq$day) 

dygraph(x_dailyq, main="Daily Temperature") %>%
  dySeries(
    c("temp_c_min",
      "temp_c_avg",
      "temp_c_max"),
    label = " ",
    color = "orangered") %>% 
  dyAxis("y", label = " ") %>% 
  dySeries(
    c("temp_c_q10",
      "temp_c_avg",
      "temp_c_q90"),
    label = "avg ºC",
    color = "orangered") %>% 
  dyOptions(
    fillGraph = FALSE, fillAlpha = 0.4) %>%
  dyRangeSelector() 
```

Daily average temperature (red line) with shading to indicate min/max (lightest orange) and quantiles 10% and 90% (darker orange).




# MARINe: Reproducible Smoothing 

### Read MARINe site data & associate with corresponding p2p site data

#### Get [Pole to Pole](https://marinebon.org/p2p) sites & convert to sf

```{r}
d_psites <- read_csv(here::here("data/p2p_sites.csv"))

d_psites_sf <- d_psites %>% 
  st_as_sf(
    coords = c("lon", "lat"), remove = F,
    crs    = 4326) # geographic coordinate ref system

# map of all p2p sites
# leaflet(d_psites_sf) %>% 
#   addProviderTiles(providers$Esri.OceanBasemap) %>% 
#   addMarkers()
```

#### Get MARINe sites, prep for combining temp data txts, & convert to sf

```{r}
# from robo metadata
d_msites <- read_csv("Robomussel metadata.csv") %>% 
  rename(
    microsite_id   = `microsite id`,
    logger_type    = `logger type`,
    tidal_height_m = `tidal height (m)`,
    wave_exposure  = `wave exposure`,
    start_date     = `start date`,
    end_date       = `end date`)
  
d_mfiles  <- tibble(
  path            = list.files(dir_gdata, ".*\\.txt", full.names = T),
  file            = basename(path),
  microsite_year  = file %>% path_ext_remove()) %>% 
  separate(microsite_year, c("msite", "year"), "_", convert = T)

# join file names with their associated metadata
d_msites <- d_mfiles %>% 
  left_join(
    d_msites, by = c("msite" = "microsite_id")) %>% 
  drop_na() # sum(is.na(d_msites$site)): n = 2

# convert to sf for joining with p2p
d_msites_sf <- d_msites %>% 
  st_as_sf(
    coords = c("longitude", "latitude"), remove = F,
    crs    = 4326) # geographic coordinate ref system (WGS1984)
```

#### Join MARINe and p2p sites by nearest feature

```{r}
# join 4 nearby sites by nearest feature
# (sites that are both MARINe and p2p that we have data for)
sites_joined <- st_join(
  d_msites_sf, 
  d_psites_sf %>% 
    select(-country) %>% 
    rename(pgeometry = geometry), 
  join = st_nearest_feature)

# function to convert sf to st_point 
# & add sf geometry list column & coord ref sys
xy2pt <- function(x, y){
  st_point(c(x, y)) %>% 
    st_sfc(crs = 4326)
}

# add MARINe & p2p geom columns,
# then use to calc distance between sites in km
sites_joined <- sites_joined %>% 
  st_drop_geometry() %>% 
  mutate(
    geom_marine = map2(longitude, latitude, xy2pt),
    geom_p2p    = map2(lon      , lat     , xy2pt),
    dist_km     = map2_dbl(geom_marine, geom_p2p, st_distance) / 1000) %>% 
  st_as_sf( 
    coords = c("lon", "lat"), remove = F,
    crs    = 4326)
# sites_joined = df of all p2p sites that are also MARINe sites for which we have data
```


### Functions for combining & smoothing MARINe temp data

```{r}
# dataCombiner updated to use for both p2p & MARINe
dataCombiner <- function(data_site_zone) {
  
  # store site and zone names
  if ("site" %in% colnames(data_site_zone)) {
    site <- unique(data_site_zone$site) 
  }
  if ("zone" %in% colnames(data_site_zone)) {
    zone <- unique(data_site_zone$zone)
  }
  
  # read temp file corresponding to each path name 
  if (file_ext(data_site_zone$path) == "csv") {
    temp_data <- bind_rows(lapply(data_site_zone$path, read_csv)) %>% 
      rename(Temp_C = sst) %>% 
      mutate(time   = parse_date_time(date, "y-m-d")) %>% 
      select(-date)

  } else {
    temp_data <- bind_rows(lapply(data_site_zone$path, read_tsv)) %>% 
      mutate(time = parse_date_time(Time_GMT, "m/d/y H:M")) %>%
      select(-Time_GMT)
  }
  
  temp_data <- temp_data %>% 
    drop_na() %>% 
    group_by(time) %>% 
    summarize(Temp_C = mean(Temp_C)) %>% 
    mutate(
      site = if("site" %in% colnames(data_site_zone)) site else NA,
      zone = if("zone" %in% colnames(data_site_zone)) zone else NA) %>% 
    relocate(site)
}
  
dailyQuantilesData <- function(data) {
  data %>% 
    mutate(
      day = floor_date(time, unit = "day")) %>%
    group_by(day) %>%
    distinct(day, .keep_all = T) %>% 
    mutate(
      temp_c_q10 = quantile(Temp_C, 0.1),
      temp_c_q90 = quantile(Temp_C, 0.9),
      temp_c_avg = mean(Temp_C),
      temp_c_min = min(Temp_C),
      temp_c_max = max(Temp_C)) %>% 
    select(-time, -Temp_C) %>% 
    gather("metric", "Temp_C", c(-1, -2, -3)) %>% 
    select(-zone, zone)
}
```

### Smooth MARINe sites

#### split by each unique site & zone combination
```{r warning=FALSE}
d_filtered <- split(sites_joined, list(sites_joined$site, sites_joined$zone), drop = T, sep = "_")
```

#### smooth temp data corresponding to each site
```{r warning=FALSE}
# loop through all site & zone combinations and store dailyq for each in a list
dailyq <- list()

for (i in 1:length(d_filtered)){ # i = 1
  
  # combine data for all zones of each site
  d_site_zone <- dataCombiner(data_site_zone = d_filtered[[i]])
  
  # get site and zone names
  site <- unique(d_filtered[[i]][["site"]])
  if ("zone" %in% colnames(d_filtered[[i]])) {
    zone <- unique(d_filtered[[i]][["zone"]])
    site_zone <-  paste0(site, "_", zone)
  } else {
    zone <- ""
    site_zone <- site
  }
  
  # smooth data
  d_dailyq <- dailyQuantilesData(d_site_zone)

  # assign global names to local objects
  stringname <- paste0(site_zone, "_dailyq")
  
  # populate dailyq list
  dailyq[[stringname]] <- d_dailyq
  
}

# Create vector of ordered zones for when we convert zones to factor
zones <- c("Low", "Lower-Mid", "Mid", "Upper-Mid", "High")

# Bind temps for all sites & zones
d <- dailyq %>% 
  bind_rows %>% 
  mutate(
    site = as.factor(site),
    zone = factor(zone, levels = zones, ordered = T))
```

#### write smoothed avg temp data for each site to a unique csv
```{r warning=FALSE}
# write csvs
for (i in 1:length(levels(d$site))) { # for each site i

  site <- levels(d$site)[i]

  d_site <- d %>% 
    filter(site == !!site) %>%
    ungroup()
  
  # Filter out avgs
  d_site_avg <- d_site %>% 
    filter(metric == "temp_c_avg") %>% 
    select(-site, -metric) %>% 
    mutate(
      zone = factor(zone, zones, ordered = T)) %>% 
    arrange(zone, day) %>% 
    pivot_wider(day, names_from = zone, values_from = Temp_C) %>% 
    arrange(day) 
  
  write_csv(
    d_site_avg,
    paste0(getwd(), "/data_smoothed/", site, ".csv"))
  
}

```


### xts conversion

```{r}
smoothed_files <- tibble(
  path = list.files(paste0(getwd(),"/data_smoothed/"), full.names = T),
  file = basename(path),
  site = file %>% path_ext_remove())

# for each file name in list of files
for (i in 1:length(smoothed_files$path)) {
  # read in associated smoothed temp data
  d_smoothed <- read_csv(smoothed_files$path[i]) %>% 
    # select(-X1) %>% 
    mutate(
      day = parse_date_time(day, "y-m-d")) %>% 
    mutate_at(vars(-1), funs(as.numeric))
  # convert to xts
  x_smoothed <- xts(select(d_smoothed, -day), order.by = d_smoothed$day)
  assign(paste0("x_", smoothed_files$site[i]), x_smoothed)
}
```

### dygraph plot for a MARINe site (avg only)
```{r}
smoothed_files <- smoothed_files %>% 
  mutate(xts = paste0("x_", site))

# Color palette for zones
pal  <- c("#3D2C9A", "#3E98C5", "#4A9A78", "#F7BD33", "#D74B00")
cols <- setNames(pal, zones) # color palette mapped to zone names

dygraph(eval(parse(text = smoothed_files$xts[1])), main = "Daily Temperature") %>%
  dyHighlight(
    highlightCircleSize = 5, 
    highlightSeriesBackgroundAlpha = 0.2,
    hideOnMouseOut = TRUE) %>%
    dyOptions(
      colors = as.character(cols[names(x_avg)])) %>% 
  dyOptions(
    fillGraph = FALSE, fillAlpha = 0.4) %>%
  dyRangeSelector() 

# dygraph function
dyPlot <- function(xts_name, colors) {
  dygraph <- 
    dygraph(eval(parse(text = xts_name)), main = "Daily Temperature") %>%
      dyHighlight(
        highlightCircleSize = 5, 
        highlightSeriesBackgroundAlpha = 0.2,
        hideOnMouseOut = TRUE) %>%
        dyOptions(
          # use only colors corresponding to zones that exist in the site's xts data
          colors = as.character(
            colors[names(eval(parse(text = xts_name)))])) %>% 
      dyOptions(
        fillGraph = FALSE, fillAlpha = 0.4) %>%
      dyRangeSelector()
  return(dygraph)
}

# example:
# dyPlot(smoothed_files$xts[3], colors = cols)

for (i in 1:length(smoothed_files$xts)) {
  dygraph <- dyPlot(smoothed_files$xts[i], colors = cols)
  assign(paste0("dy_", smoothed_files$site[i]), dygraph)
}

dy_USCABD
dy_USCACA
dy_USCAHS
dy_USCAPD
```


# P2P (non-MARINe sites)

### Read p2p site data (that don't have corresponding MARINe sites)

#### prep directories
```{r}
dir_p2p_robo <-
  case_when(
    user == "bbest"       ~ "/Volumes/GoogleDrive/My Drive/projects/mbon-p2p/Robolimpets",
    user == "cdobbelaere" ~ here::here("../Robolimpets"))
  
paths <- list.files(
  dir_p2p_robo, ".*\\.csv$", 
  recursive = T, full.names = T,
  include.dirs = T) # 14 CSVs
```

#### functions to read in p2p temp data & metadata
```{r warning=FALSE}
# find where metadata ends
find_skip <- function(file, pattern, n = 20) { 
  min(grep(pattern, read_lines(file, n_max = n)))
}

# function to read temp csv files
read_tempcsv <- function(path) {
  data <- tibble(
    read_csv(
      path,
      skip = (find_skip(file = path, pattern = "^time,") - 1))) %>% 
    drop_na() 
  if ("temp" %in% names(data)) {
    data <- data %>% rename(Temp_C = temp)
  }
  if (TRUE %in% grepl("-", data$time)) {
    data <- data %>% mutate(time = parse_date_time(time,"y-m-d H:M:S"))
  } else if (TRUE %in% grepl("/", data$time)) {
    data <- data %>% mutate(time = parse_date_time(time, "m/d/y H:M"))
  }
  data <- data %>% 
    mutate(
      site = 
        if ("site" %in% colnames(data)) site 
        else if ("sensor" %in% colnames(data)) sensor 
        else NA,
      zone = if ("zone" %in% colnames(data)) zone else NA)
  if ("sensor" %in% colnames(data)) {data <- data %>% select(-sensor)}
  return(data)
}

# read and clean metadata
read_metadata <- function(path) { 
  
  # if metadata present:
  if (find_skip(file = path, pattern = "^time,") != Inf) {
    metadata <- tibble(
      metadata = read_lines(
        path,
        n_max = (find_skip(file = path, pattern = "^time,") - 1))) %>% 
      mutate_if(is.character, function(x) {
        Encoding(x) <- "latin1"; return(x) 
        }) %>% 
      filter(str_detect(metadata, ":")) %>% 
      separate(metadata, c("key", "value"), ": ", convert = T) %>% 
      pivot_wider(names_from = "key", values_from = "value")
    
    if ("custom name" %in% colnames(metadata)) {
      metadata <- metadata %>% select(-"custom name")
    }

    if ("coords" %in% colnames(metadata)) {
      if (TRUE %in% grepl(",", metadata$coords)) {
         metadata <- metadata %>%
           separate(coords, c("lat", "lon"), ", ")
      } else {
        metadata <- metadata %>%
          separate(coords, c("lat", "lon"), " ")
      }
    }

    # fix coords
    if (TRUE %in% grepl("S", metadata$lat)) {
      metadata$lat <- gsub("S", "", metadata$lat)
      metadata <- metadata %>%
        mutate(lat = as.double(paste0("-", metadata$lat)))
    } else if (TRUE %in% grepl("N", metadata$lat)) {
      metadata$lat <- metadata$lat %>%
        gsub("N", "", .) %>%
        as.double()
    }

    if (TRUE %in% grepl("W", metadata$lon)) {
      metadata$lon <- metadata$lon %>%
        as.double(paste0("-", metadata$lon)) %>%
        gsub("W", "", .)
    } else if (TRUE %in% grepl("E", metadata$lon)) {
      metadata$lon <- metadata$lon %>%
        gsub("E", "", .) %>%
        as.double()
    }

    metadata <- metadata %>%
      mutate_all(list(~gsub(",", "", .)))
  }

  if ("X1" %in% colnames(metadata)) {
    metadata <- metadata %>% select(-X1)
  }

  if (TRUE %in% grepl("ample", names(metadata))) {
    metadata <- metadata %>%
      rename(number_of_samples = grep("ample", names(metadata), value = T))
  }

  metadata <- metadata %>% mutate(across(!`serial number`, as.numeric))
  return(metadata)
}

```



```{r warning=FALSE}
# read all csv files in 'robolimpets' folder and combine data
d_p2p <- tibble(
  path     = paths,
  file     = basename(path),
  data     = map(path, read_tempcsv)) 
  # to filter out only temp data and not include metadata csvs
  # filter(map_lgl(data, ~ncol(.) == 2))

# datapoints 7, 10, 13 are irregular

metadata <- map(d_p2p$path, read_metadata) %>% 
  bind_rows 

metadata <- tibble(metadata, path = d_p2p$path) %>% 
  select(path, everything())

d_p2p <- d_p2p %>% left_join(metadata, by = c("path" = "path"))

library(readxl)
p2p_meta <- readxl::read_excel(paste0(dir_p2p_robo, "/MBON_Pole-to-Pole_logger_list.xlsx"))

p2p_meta$`Serial Number` <- p2p_meta$`Serial Number` %>% 
  gsub("-", " ", . )

list.files()





hello <- left_join(d_p2p, p2p_meta, by = c("serial number" = "Serial Number"))



# copied from before:

# convert d_p2p to sf
d_p2p_sf <- d_p2p %>% 
  drop_na() %>% # for now..
  st_as_sf(coords = c("lat", "lon"), remove = F) %>% 
  st_set_crs(4326) # problem with coords...not correct
View(d_p2p_sf)

leaflet(d_p2p_sf) %>% addTiles() %>%   addMarkers()


# join 4 nearby sites by nearest feature
# (sites that are both MARINe and p2p that we have data for)
p2p_sites_joined <- st_join(
  d_msites_sf, 
  d_psites_sf %>% 
    select(-country) %>% 
    rename(pgeometry = geometry), 
  join = st_nearest_feature)

# function to convert sf to st_point 
# & add sf geometry list column & coord ref sys
xy2pt <- function(x, y){
  st_point(c(x, y)) %>% 
    st_sfc(crs = 4326)
}

# add MARINe & p2p geom columns,
# then use to calc distance between sites in km
sites_joined <- sites_joined %>% 
  st_drop_geometry() %>% 
  mutate(
    geom_marine = map2(longitude, latitude, xy2pt),
    geom_p2p    = map2(lon      , lat     , xy2pt),
    dist_km     = map2_dbl(geom_marine, geom_p2p, st_distance) / 1000) %>% 
  st_as_sf( 
    coords = c("lon", "lat"), remove = F,
    crs    = 4326)
```







```{r warning=FALSE}
# For each site i, write csv of smoothed temp data for all zones
for (i in 1:length(d$site)) { # for each site i

  # smooth data
  # d_dailyq <- dailyQuantilesData(d_p2p[["data"]][[i]])

  
  
  
  # Filter out avgs only
  d_site_avg <- d_site %>% 
    filter(metric == "temp_c_avg") %>% 
    select(-site, -metric) %>% 
    # mutate(
    #   zone = factor(zone, zones, ordered = T)) %>% 
    arrange(zone, day) %>% 
    pivot_wider(day, names_from = zone, values_from = Temp_C) %>% 
    arrange(day) 
  
  write_csv(
    d_dailyq,
    paste0(getwd(), "/data_smoothed/", site, ".csv"))
  
}
```


```{r warning=FALSE}
# For each site i, write csv of smoothed temp data for all zones
for (i in 1:length(d_p2p$path)) { # for each site i

  # smooth data
  d_dailyq <- dailyQuantilesData(d_p2p[["data"]][[i]])
  
  # Filter out avgs only
  d_site_avg <- d_site %>% 
    filter(metric == "temp_c_avg") %>% 
    select(-site, -metric) %>% 
    # mutate(
    #   zone = factor(zone, zones, ordered = T)) %>% 
    arrange(zone, day) %>% 
    pivot_wider(day, names_from = zone, values_from = Temp_C) %>% 
    arrange(day) 
  
  write_csv(
    d_dailyq,
    paste0(getwd(), "/data_smoothed/", site, ".csv"))
  
}




p_dailyq <- list()
p_dailyq[1] <- dailyQuantilesData(d_p2p[["data"]][[1]])

```






















## NEW SMOOTHING FNS
```{r}
colnames(sites_joined)

h <- bind_rows(sites_joined, d_p2p)
View(h)



d_filtered <- split(sites_joined, list(sites_joined$site, sites_joined$zone), drop = T, sep = "_")

d_p2p_filtered <- split(d_p2p, d_p2p$file, drop = T, sep = "_")

View(d_p2p_filtered)



# trying to make a datacombiner that's reproducible across ALL sites 

# dataCombiner updated to use for both p2p & MARINe
# don't need to run for d_p2p since already combined
dataCombiner <- function(data_site_zone) {
  
  # store site and zone names
  # for MARINe:
  if ("site" %in% colnames(data_site_zone)) {
    site <- unique(data_site_zone$site) 
  } 
  if ("zone" %in% colnames(data_site_zone)) {
    zone <- unique(data_site_zone$zone)
  }
  
  # read temp file corresponding to each path name 
  if (file_ext(data_site_zone$path) == "csv") {
    temp_data <- bind_rows(lapply(data_site_zone$path, read_csv)) %>% 
      rename(Temp_C = sst) %>% 
      mutate(time   = parse_date_time(date, "y-m-d")) %>% 
      select(-date)

  } else {
    temp_data <- bind_rows(lapply(data_site_zone$path, read_tsv)) %>% 
      mutate(time = parse_date_time(Time_GMT, "m/d/y H:M")) %>%
      select(-Time_GMT)
  }
  
  temp_data <- temp_data %>% 
    drop_na() %>% 
    group_by(time) %>% 
    summarize(Temp_C = mean(Temp_C)) %>% 
    mutate(
      site = if("site" %in% colnames(data_site_zone)) site else NA,
      zone = if("zone" %in% colnames(data_site_zone)) zone else NA) %>% 
    relocate(site)
}
  
dailyQuantilesData <- function(data) {
  
  smoothed_data <- data %>% 
    mutate(
      day = lubridate::floor_date(time, unit = "day")) %>%
    group_by(day) %>%
    # distinct(day, .keep_all = T) %>% 
    mutate(
      temp_c_q10 = quantile(Temp_C, 0.1),
      temp_c_q90 = quantile(Temp_C, 0.9),
      temp_c_avg = mean(Temp_C),
      temp_c_min = min(Temp_C),
      temp_c_max = max(Temp_C)) %>% 
    ungroup() %>% 
    distinct(day, .keep_all = T) %>% 
    select(-time, -Temp_C) %>% 
    gather("metric", "Temp_C", c(-1, -2, -3))
  # if ("zone" %in% colnames(smoothed_data)) {
  #   select(-zone, zone)
    # return(smoothed_data)
  return(smoothed_data)
  
}

dailyQuantilesData(d_p2p[["data"]][[9]])

dailyQuantilesData()

dailyQuantilesData(d_p2p[["data"]][[4]])


data_1 <- d_p2p[["data"]][[9]] 
# %>% 
  # mutate(time = parse_date_time(time, "y-m-d H:M:S"))
  #        

smoothed_1 <- dailyQuantilesData(data_1)

d_site_avg_1 <- smoothed_1 %>% 
  pivot_wider(day, names_from = metric, values_from = Temp_C)


x_smoothed_1 <- xts(select(d_site_avg_1, -day), order.by = d_site_avg_1$day)
View(x_smoothed_1)

dygraph(x_smoothed_1, main="Daily Temperature") %>%
  dyAxis("y", label = " ") %>% 
  dySeries(
    c("temp_c_min",
      "temp_c_avg",
      "temp_c_max"),
    label = "avg ºC",
    color = "orangered") %>% 
  dyOptions(
    fillGraph = FALSE, fillAlpha = 0.4) %>%
  dyRangeSelector() 

# for (i in 1:length(levels(d$site))) { # for each site i

  # site <- levels(d$site)[i]

  # d_site <- smoothed_1 %>% 
  #   filter(site == !!site) %>%
  #   ungroup()
  
  # Filter out avgs
  d_site_avg_1 <- smoothed_1 %>% 
    filter(metric == "temp_c_avg") %>% 
    select(-site, -metric) %>% 
    mutate(
      zone = factor(zone, zones, ordered = T)) %>% 
    arrange(zone, day) %>% 
    pivot_wider(day, names_from = zone, values_from = Temp_C) %>% 
    arrange(day) 
  
  d_site_avg_1 <- smoothed_1 %>% 
    pivot_wider(day, names_from = metric, values_from = Temp_C)
  xts_1 <- 
  
  
  
  
  
  write_csv(
    d_site_avg,
    paste0(getwd(), "/data_smoothed/", site, ".csv"))
  
}


View(d_site_avg_1)

















View(smoothed_1)



View(d_p2p_filtered[["data"]][[1]])

dailyQuantilesData(d_p2p_filtered[[1]])
```


### Smooth MARINe 




### Smooth p2p (only 1 zone per site)

```{r}
# split by each unique site & zone combination
# d_filtered <- split(d_sites, list(d_sites$site, d_sites$zone), drop = T, sep = "_")
# d_filtered <- split(sites_joined, list(sites_joined$site, sites_joined$zone), drop = T, sep = "_")

# loop through all site & zone combinations and store dailyq for each in a list
dailyq_p2p <- list()



# don't forget to exclude metadata (last logger)
# & look at folder names to access real site names (instead of just serial nums)


for (i in 1:(nrow(d_p2p) - 1){ # i = 1
  

  # d_site_zone <- dataCombiner(data_site_zone = d_p2p[[i]])
  
  
  
  # get site and zone names
  site <- path_ext_remove(d_p2p$file)
  site_zone <- site
  
  # site <- unique(d_filtered[[i]][["site"]])
  
  
  # if ("zone" %in% colnames(d_filtered[[i]])) {
  #   zone <- unique(d_filtered[[i]][["zone"]])
  #   site_zone <-  paste0(site, "_", zone)
  # } else {
  #   zone <- ""
  #   site_zone <- site
  # }
  
  # smooth data
  d_dailyq <- dailyQuantilesData(d_p2p$data)
  
  
  
  
  
  
  # x_dailyq <- dailyQuantiles_xts(d_site_zone)
  
  # assign global names to local objects
  stringname <- paste0(site_zone, "_dailyq")
  
  dailyq[[stringname]] <- d_dailyq
  
  # will write csv later once bound by site
  # write_csv(
  #   d_dailyq,
  #   path = paste0(getwd(), "/data_smoothed/", stringname, ".csv"))
 
}






# loop through all site & zone combinations and store dailyq for each in a list
dailyq_p2p <- list()

for (i in 1:length(d_filtered)){ # i = 1
  
  # combine data for all zones of each site
  d_site_zone <- dataCombiner(data_site_zone = d_filtered[[i]])
  
  # get site and zone names
  site <- unique(d_filtered[[i]][["site"]])
  if ("zone" %in% colnames(d_filtered[[i]])) {
    zone <- unique(d_filtered[[i]][["zone"]])
    site_zone <-  paste0(site, "_", zone)
  } else {
    zone <- ""
    site_zone <- site
  }
  
  # smooth data
  d_dailyq <- dailyQuantilesData(d_site_zone)
  # x_dailyq <- dailyQuantiles_xts(d_site_zone)
  
  # assign global names to local objects
  stringname <- paste0(site_zone, "_dailyq")
  
  dailyq[[stringname]] <- d_dailyq
  
  # will write csv later once bound by site
  # write_csv(
  #   d_dailyq,
  #   path = paste0(getwd(), "/data_smoothed/", stringname, ".csv"))
 
}

```








