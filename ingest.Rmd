---
title: "ingest"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, message = F, warning = F)
```

## TODO

1. PROBLEM. Data is too big for simple interactive time-series plot (ie dygraphs) if on same Rmd output html page. The folder of *.csv's [robomusseldata20201030 - Google Drive](https://drive.google.com/drive/u/3/folders/1kzjZ72vFxRGsafTgCCzq6cXIhQ1GYQa3) contains 317 files totalling 236 MB. SOLUTION(s):
    1. Read into SQLite database and show as Shiny app.

  

# Load packages & user info

```{r}
source(here::here("functions.R"))
```

### MARIne report output

### p2p gdata report output

### Read p2p gdata

#### prep directories
```{r}
dir_p2p_robo <-
  case_when(
    user == "bbest"       ~ "/Volumes/GoogleDrive/My Drive/projects/mbon-p2p/Robolimpets",
    user == "cdobbelaere" ~ here::here("../Robolimpets"))
  
paths <- list.files(
  dir_p2p_robo, ".*\\.csv$", 
  recursive = T, full.names = T,
  include.dirs = T) # 14 CSVs
```

#### functions to read in gdata temp data & metadata
```{r warning=FALSE}
# find where metadata ends
find_skip <- function(file, pattern, n = 20) { 
  min(grep(pattern, read_lines(file, n_max = n)))
}

# function to read temp csv files
read_tempcsv <- function(path) {
  data <- tibble(
    read_csv(
      path,
      skip = (find_skip(file = path, pattern = "^time,") - 1))) %>% 
    drop_na() 
  if ("temp" %in% names(data)) {
    data <- data %>% rename(Temp_C = temp)
  }
  if (TRUE %in% grepl("-", data$time)) {
    data <- data %>% mutate(time = parse_date_time(time,"y-m-d H:M:S"))
  } else if (TRUE %in% grepl("/", data$time)) {
    data <- data %>% mutate(time = parse_date_time(time, "m/d/y H:M"))
  }
  data <- data %>% 
    mutate(
      site = 
        if ("site" %in% colnames(data)) site 
        else if ("sensor" %in% colnames(data)) sensor 
        else NA,
      zone = if ("zone" %in% colnames(data)) zone else NA)
  if ("sensor" %in% colnames(data)) {data <- data %>% select(-sensor)}
  return(data)
}
```


```{r warning=FALSE}
# read and clean metadata
read_metadata <- function(path) { 
  
  n_start <- find_skip(file = path, pattern = "^time,")
  
  # if metadata present:
  if (n_start != Inf) {
    
    metadata <- tibble(
      raw_data = read_lines(
        path,
        n_max = n_start - 1)) 

    metadata <- metadata %>% 
      mutate_if(
        is.character, 
        function(x) {Encoding(x) <- "latin1"; return(x)}) %>% 
      filter(str_detect(raw_data, ":")) %>% 
      separate(raw_data, c("key", "value"), ": ", convert = T) %>% 
      pivot_wider(names_from = "key", values_from = "value")
    
    if ("custom name" %in% names(metadata)) {
      metadata <- metadata %>% select(-"custom name")
    }

    if ("coords" %in% names(metadata)) {
      if (TRUE %in% grepl(",", metadata$coords)) {
         metadata <- metadata %>%
           separate(coords, c("lat", "lon"), ", ")
      } else {
        metadata <- metadata %>%
          separate(coords, c("lat", "lon"), " ")
      }
    }

    # fix coords
    if (TRUE %in% grepl("S", metadata$lat)) {
      metadata$lat <- gsub("S", "", metadata$lat)
      metadata <- metadata %>% mutate(lat = glue("-{metadata$lat}"))
    } else if (TRUE %in% grepl("N", metadata$lat)) {
      metadata$lat <- metadata$lat %>%
        gsub("N", "", .) 
        # as.double()
    }
    
    if (TRUE %in% grepl("E", metadata$lon)) {
      metadata$lon <- gsub("E", "", metadata$lon)
      metadata <- metadata %>% mutate(lon = glue("-{metadata$lon}"))
    } else if (TRUE %in% grepl("W", metadata$lon)) {
      metadata$lon <- metadata$lon %>% 
        gsub("W", "", .) 
        # as.double()
    }
      
    metadata <- metadata %>%
      mutate_all(list(~gsub(",", "", .))) %>% 
      select(-"accuracy (m)")
 
  if ("X1" %in% colnames(metadata)) {
    metadata <- metadata %>% select(-X1)
  }

  if (TRUE %in% grepl("ample", names(metadata))) {
    metadata <- metadata %>%
      rename(number_of_samples = grep("ample", names(metadata), value = T))
  }

  # remove non-numeric characters
  metadata <- metadata %>% mutate(across(!`serial number`, as.numeric)) 
  
  # convert back to character for filling NAs
  metadata <- metadata %>% 
    mutate(across(everything(), as.character)) 
  }
  return(metadata)
}
```



```{r warning=FALSE}
# read all csv files in 'robolimpets' folder and combine data
d_p2p <- tibble(
  path     = paths,
  file     = basename(path),
  data     = map(path, read_tempcsv)) 

metadata <- tibble()
metadata <- map(d_p2p$path, read_metadata) %>% 
  bind_rows

# fill NAs
for (i in 1:nrow(metadata)) {
  for (j in 1:ncol(metadata)) {
    # if there's an NA
    if (is.na(metadata[i, j])) {
      # find if serial number matches any other serial number
      if (TRUE %in% grepl(as.character(metadata$`serial number`[i]), metadata$`serial number`)) {
        row <- (grep(as.character(metadata$`serial number`[i]), metadata$`serial number`))[1]
        # replace that row's NAs with the value of the other rows
        metadata[i, j] <- metadata[row, j]
      }
    }
  }
}

metadata <- metadata %>% mutate(across(!`serial number`, as.numeric))


meta_psites_sf <- metadata %>% 
  st_as_sf(
    coords = c("lon", "lat"), remove = F,
    crs    = 4326) # geographic coordinate ref system
View(meta_psites_sf)


psites_joined <- st_join(
  meta_psites_sf, 
  d_psites_sf %>% 
    rename(pgeometry = geometry), 
  join = st_nearest_feature) %>% 
  left_join(d_p2p, by = ) # need to finish this -- by adding path col to d_p2p and joining by that









```

```{r warning=FALSE}





# copied from before:

# convert d_p2p to sf
d_p2p_sf <- d_p2p %>% 
  drop_na() %>% # for now..
  st_as_sf(coords = c("lat", "lon"), remove = F) %>% 
  st_set_crs(4326) # problem with coords...not correct
View(d_p2p_sf)

leaflet(d_p2p_sf) %>% addTiles() %>%   addMarkers()


# join 4 nearby sites by nearest feature
# (sites that are both MARINe and p2p that we have data for)
p2p_sites_joined <- st_join(
  d_msites_sf, 
  d_psites_sf %>% 
    select(-country) %>% 
    rename(pgeometry = geometry), 
  join = st_nearest_feature)

# function to convert sf to st_point 
# & add sf geometry list column & coord ref sys
xy2pt <- function(x, y){
  st_point(c(x, y)) %>% 
    st_sfc(crs = 4326)
}

# add MARINe & p2p geom columns,
# then use to calc distance between sites in km
sites_joined <- sites_joined %>% 
  st_drop_geometry() %>% 
  mutate(
    geom_marine = map2(longitude, latitude, xy2pt),
    geom_p2p    = map2(lon      , lat     , xy2pt),
    dist_km     = map2_dbl(geom_marine, geom_p2p, st_distance) / 1000) %>% 
  st_as_sf( 
    coords = c("lon", "lat"), remove = F,
    crs    = 4326)
```







```{r warning=FALSE}
# For each site i, write csv of smoothed temp data for all zones
for (i in 1:length(d$site)) { # for each site i

  # smooth data
  # d_dailyq <- dailyQuantilesData(d_p2p[["data"]][[i]])

  
  
  
  # Filter out avgs only
  d_site_avg <- d_site %>% 
    filter(metric == "temp_c_avg") %>% 
    select(-site, -metric) %>% 
    # mutate(
    #   zone = factor(zone, zones, ordered = T)) %>% 
    arrange(zone, day) %>% 
    pivot_wider(day, names_from = zone, values_from = Temp_C) %>% 
    arrange(day) 
  
  write_csv(
    d_dailyq,
    paste0(getwd(), "/data_smoothed/", site, ".csv"))
  
}
```


```{r warning=FALSE}
# For each site i, write csv of smoothed temp data for all zones
for (i in 1:length(d_p2p$path)) { # for each site i

  # smooth data
  d_dailyq <- dailyQuantilesData(d_p2p[["data"]][[i]])
  
  # Filter out avgs only
  d_site_avg <- d_site %>% 
    filter(metric == "temp_c_avg") %>% 
    select(-site, -metric) %>% 
    # mutate(
    #   zone = factor(zone, zones, ordered = T)) %>% 
    arrange(zone, day) %>% 
    pivot_wider(day, names_from = zone, values_from = Temp_C) %>% 
    arrange(day) 
  
  write_csv(
    d_dailyq,
    paste0(getwd(), "/data_smoothed/", site, ".csv"))
  
}




p_dailyq <- list()
p_dailyq[1] <- dailyQuantilesData(d_p2p[["data"]][[1]])

```






















## NEW SMOOTHING FNS
```{r}
colnames(sites_joined)

h <- bind_rows(sites_joined, d_p2p)
View(h)



d_filtered <- split(sites_joined, list(sites_joined$site, sites_joined$zone), drop = T, sep = "_")

d_p2p_filtered <- split(d_p2p, d_p2p$file, drop = T, sep = "_")

View(d_p2p_filtered)



# trying to make a datacombiner that's reproducible across ALL sites 

# dataCombiner updated to use for both p2p & MARINe
# don't need to run for d_p2p since already combined
dataCombiner <- function(data_site_zone) {
  
  # store site and zone names
  # for MARINe:
  if ("site" %in% colnames(data_site_zone)) {
    site <- unique(data_site_zone$site) 
  } 
  if ("zone" %in% colnames(data_site_zone)) {
    zone <- unique(data_site_zone$zone)
  }
  
  # read temp file corresponding to each path name 
  if (file_ext(data_site_zone$path) == "csv") {
    temp_data <- bind_rows(lapply(data_site_zone$path, read_csv)) %>% 
      rename(Temp_C = sst) %>% 
      mutate(time   = parse_date_time(date, "y-m-d")) %>% 
      select(-date)

  } else {
    temp_data <- bind_rows(lapply(data_site_zone$path, read_tsv)) %>% 
      mutate(time = parse_date_time(Time_GMT, "m/d/y H:M")) %>%
      select(-Time_GMT)
  }
  
  temp_data <- temp_data %>% 
    drop_na() %>% 
    group_by(time) %>% 
    summarize(Temp_C = mean(Temp_C)) %>% 
    mutate(
      site = if("site" %in% colnames(data_site_zone)) site else NA,
      zone = if("zone" %in% colnames(data_site_zone)) zone else NA) %>% 
    relocate(site)
}
  
dailyQuantilesData <- function(data) {
  
  smoothed_data <- data %>% 
    mutate(
      day = lubridate::floor_date(time, unit = "day")) %>%
    group_by(day) %>%
    # distinct(day, .keep_all = T) %>% 
    mutate(
      temp_c_q10 = quantile(Temp_C, 0.1),
      temp_c_q90 = quantile(Temp_C, 0.9),
      temp_c_avg = mean(Temp_C),
      temp_c_min = min(Temp_C),
      temp_c_max = max(Temp_C)) %>% 
    ungroup() %>% 
    distinct(day, .keep_all = T) %>% 
    select(-time, -Temp_C) %>% 
    gather("metric", "Temp_C", c(-1, -2, -3))
  # if ("zone" %in% colnames(smoothed_data)) {
  #   select(-zone, zone)
    # return(smoothed_data)
  return(smoothed_data)
  
}

dailyQuantilesData(d_p2p[["data"]][[9]])

dailyQuantilesData()

dailyQuantilesData(d_p2p[["data"]][[4]])


data_1 <- d_p2p[["data"]][[9]] 
# %>% 
  # mutate(time = parse_date_time(time, "y-m-d H:M:S"))
  #        

smoothed_1 <- dailyQuantilesData(data_1)

d_site_avg_1 <- smoothed_1 %>% 
  pivot_wider(day, names_from = metric, values_from = Temp_C)


x_smoothed_1 <- xts(select(d_site_avg_1, -day), order.by = d_site_avg_1$day)
View(x_smoothed_1)

dygraph(x_smoothed_1, main="Daily Temperature") %>%
  dyAxis("y", label = " ") %>% 
  dySeries(
    c("temp_c_min",
      "temp_c_avg",
      "temp_c_max"),
    label = "avg ÂºC",
    color = "orangered") %>% 
  dyOptions(
    fillGraph = FALSE, fillAlpha = 0.4) %>%
  dyRangeSelector() 

# for (i in 1:length(levels(d$site))) { # for each site i

  # site <- levels(d$site)[i]

  # d_site <- smoothed_1 %>% 
  #   filter(site == !!site) %>%
  #   ungroup()
  
  # Filter out avgs
  d_site_avg_1 <- smoothed_1 %>% 
    filter(metric == "temp_c_avg") %>% 
    select(-site, -metric) %>% 
    mutate(
      zone = factor(zone, zones, ordered = T)) %>% 
    arrange(zone, day) %>% 
    pivot_wider(day, names_from = zone, values_from = Temp_C) %>% 
    arrange(day) 
  
  d_site_avg_1 <- smoothed_1 %>% 
    pivot_wider(day, names_from = metric, values_from = Temp_C)
  xts_1 <- 
  
  
  
  
  
  write_csv(
    d_site_avg,
    paste0(getwd(), "/data_smoothed/", site, ".csv"))
  
}


View(d_site_avg_1)

















View(smoothed_1)



View(d_p2p_filtered[["data"]][[1]])

dailyQuantilesData(d_p2p_filtered[[1]])
```


### Smooth MARINe 




### Smooth p2p (only 1 zone per site)

```{r}
# split by each unique site & zone combination
# d_filtered <- split(d_sites, list(d_sites$site, d_sites$zone), drop = T, sep = "_")
# d_filtered <- split(sites_joined, list(sites_joined$site, sites_joined$zone), drop = T, sep = "_")

# loop through all site & zone combinations and store dailyq for each in a list
dailyq_p2p <- list()



# don't forget to exclude metadata (last logger)
# & look at folder names to access real site names (instead of just serial nums)


for (i in 1:(nrow(d_p2p) - 1){ # i = 1
  

  # d_site_zone <- dataCombiner(data_site_zone = d_p2p[[i]])
  
  
  
  # get site and zone names
  site <- path_ext_remove(d_p2p$file)
  site_zone <- site
  
  # site <- unique(d_filtered[[i]][["site"]])
  
  
  # if ("zone" %in% colnames(d_filtered[[i]])) {
  #   zone <- unique(d_filtered[[i]][["zone"]])
  #   site_zone <-  paste0(site, "_", zone)
  # } else {
  #   zone <- ""
  #   site_zone <- site
  # }
  
  # smooth data
  d_dailyq <- dailyQuantilesData(d_p2p$data)
  
  
  
  
  
  
  # x_dailyq <- dailyQuantiles_xts(d_site_zone)
  
  # assign global names to local objects
  stringname <- paste0(site_zone, "_dailyq")
  
  dailyq[[stringname]] <- d_dailyq
  
  # will write csv later once bound by site
  # write_csv(
  #   d_dailyq,
  #   path = paste0(getwd(), "/data_smoothed/", stringname, ".csv"))
 
}






# loop through all site & zone combinations and store dailyq for each in a list
dailyq_p2p <- list()

for (i in 1:length(d_filtered)){ # i = 1
  
  # combine data for all zones of each site
  d_site_zone <- dataCombiner(data_site_zone = d_filtered[[i]])
  
  # get site and zone names
  site <- unique(d_filtered[[i]][["site"]])
  if ("zone" %in% colnames(d_filtered[[i]])) {
    zone <- unique(d_filtered[[i]][["zone"]])
    site_zone <-  paste0(site, "_", zone)
  } else {
    zone <- ""
    site_zone <- site
  }
  
  # smooth data
  d_dailyq <- dailyQuantilesData(d_site_zone)
  # x_dailyq <- dailyQuantiles_xts(d_site_zone)
  
  # assign global names to local objects
  stringname <- paste0(site_zone, "_dailyq")
  
  dailyq[[stringname]] <- d_dailyq
  
  # will write csv later once bound by site
  # write_csv(
  #   d_dailyq,
  #   path = paste0(getwd(), "/data_smoothed/", stringname, ".csv"))
 
}

```








